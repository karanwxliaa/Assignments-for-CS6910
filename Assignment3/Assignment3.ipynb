{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ASSIGNMENT3\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cvBkd07iYZxG"
      },
      "source": [
        "## 1 Downloading the **Dakshina** dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iQ5BwqDIYSP8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb==0.12.2 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.12.2)\n",
            "Requirement already satisfied: pathtools in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (0.1.2)\n",
            "Requirement already satisfied: configparser>=3.8.1 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (5.3.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (3.1.29)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.13.0 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (1.16.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (2.28.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (5.9.4)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (3.19.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (8.1.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (1.0.11)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (0.4.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (3.5.4)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (2.3.0)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from wandb==0.12.2) (6.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from Click!=8.0.0,>=7.0->wandb==0.12.2) (0.4.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from GitPython>=1.0.0->wandb==0.12.2) (4.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3,>=2.0.0->wandb==0.12.2) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3,>=2.0.0->wandb==0.12.2) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3,>=2.0.0->wandb==0.12.2) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3,>=2.0.0->wandb==0.12.2) (2019.11.28)\n",
            "Requirement already satisfied: termcolor<3.0,>=2.2 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from yaspin>=1.0.0->wandb==0.12.2) (2.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\karan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.12.2) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb==0.12.2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B3KQEek0Yl0k"
      },
      "source": [
        "## 2 Processing the dataset\n",
        "### 2.1 Data Processing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6wtNj_IqYrW5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "\n",
        "class DataProcessing():\n",
        "\n",
        "    def __init__(self, path, s_lang = 'en', t_lang = \"te\"):\n",
        "    \n",
        "        self.s_lang = s_lang\n",
        "        self.t_lang = t_lang\n",
        "    \n",
        "        self.trainpath = os.path.join(path, t_lang, \"lexicons\", t_lang+\".translit.sampled.train.tsv\")\n",
        "        self.valpath = os.path.join(path, t_lang, \"lexicons\", t_lang+\".translit.sampled.dev.tsv\")\n",
        "        self.testpath = os.path.join(path, t_lang, \"lexicons\", t_lang+\".translit.sampled.test.tsv\")\n",
        "\n",
        "        self.train = pd.read_csv(\n",
        "            self.trainpath,\n",
        "            sep=\"\\t\",\n",
        "            names=[\"tgt\", \"src\", \"count\"],\n",
        "        )\n",
        "        self.test = pd.read_csv(\n",
        "            self.testpath,\n",
        "            sep=\"\\t\",\n",
        "            names=[\"tgt\", \"src\", \"count\"],\n",
        "        )\n",
        "        self.val = pd.read_csv(\n",
        "            self.valpath,\n",
        "            sep=\"\\t\",\n",
        "            names=[\"tgt\", \"src\", \"count\"],\n",
        "        )\n",
        "\n",
        "        #Train Data\n",
        "        self.train_data = self.preprocess(self.train[\"src\"].to_list(), self.train[\"tgt\"].to_list())\n",
        "        (\n",
        "            self.train_encoder_input,\n",
        "            self.train_decoder_input,\n",
        "            self.train_decoder_target,\n",
        "            self.source_voccab,\n",
        "            self.target_voccab,\n",
        "        ) = self.train_data\n",
        "\n",
        "        #character to integer and integer to character\n",
        "        self.src_charTOint, self.src_intTOchar = self.source_voccab\n",
        "        self.tar_charTOint, self.tar_intTOchar = self.target_voccab\n",
        "\n",
        "        #Validation Data\n",
        "        self.val_data = self.encode(\n",
        "            self.val[\"src\"].to_list(),\n",
        "            self.val[\"tgt\"].to_list(),\n",
        "            list(self.src_charTOint.keys()),\n",
        "            list(self.tar_charTOint.keys()),\n",
        "            src_charTOint=self.src_charTOint,\n",
        "            tar_charTOint=self.tar_charTOint,\n",
        "        )\n",
        "        self.val_encoder_input, self.val_decoder_input, self.val_decoder_target = self.val_data\n",
        "        self.src_charTOint, self.src_intTOchar = self.source_voccab\n",
        "        self.tar_charTOint, self.tar_intTOchar = self.target_voccab\n",
        "\n",
        "        #Test Data\n",
        "        self.test_data = self.encode(\n",
        "            self.test[\"src\"].to_list(),\n",
        "            self.test[\"tgt\"].to_list(),\n",
        "            list(self.src_charTOint.keys()),\n",
        "            list(self.tar_charTOint.keys()),\n",
        "            src_charTOint=self.src_charTOint,\n",
        "            tar_charTOint=self.tar_charTOint,\n",
        "        )\n",
        "        self.test_encoder_input, self.test_decoder_input, self.test_decoder_target = self.test_data\n",
        "        self.src_charTOint, self.src_intTOchar = self.source_voccab\n",
        "        self.tar_charTOint, self.tar_intTOchar = self.target_voccab\n",
        "\n",
        "\n",
        "    def dictionary_lookup(self, voccab):\n",
        "        charTOint = dict([(char, i) for i, char in enumerate(voccab)])\n",
        "        intTOchar = dict((i, char) for char, i in charTOint.items())\n",
        "        return charTOint, intTOchar\n",
        "\n",
        "    def preprocess(self, source , target):\n",
        "        source_chars = set()\n",
        "        target_chars = set()\n",
        "\n",
        "        source = [str(x) for x in source]\n",
        "        target = [str(x) for x in target]\n",
        "\n",
        "        source_words,target_words = [],[]\n",
        "        for src, tgt in zip(source, target):\n",
        "            tgt = \"\\t\" + tgt + \"\\n\"\n",
        "            \n",
        "            source_words.append(src)\n",
        "            target_words.append(tgt)\n",
        "\n",
        "            for char in src:\n",
        "                if char not in source_chars:\n",
        "                    source_chars.add(char)\n",
        "\n",
        "            for char in tgt:\n",
        "                if char not in target_chars:\n",
        "                    target_chars.add(char)\n",
        "\n",
        "        source_chars = sorted(list(source_chars))\n",
        "        target_chars = sorted(list(target_chars))\n",
        "\n",
        "        #Add space\n",
        "        source_chars.append(\" \")\n",
        "        target_chars.append(\" \")\n",
        "\n",
        "        num_encoder_tokens = len(source_chars)\n",
        "        num_decoder_tokens = len(target_chars)\n",
        "        max_source_length = max([len(txt) for txt in source_words])\n",
        "        max_target_length = max([len(txt) for txt in target_words])\n",
        "\n",
        "        print(\"No. of samples:\", len(source))\n",
        "        print(\"Src voccab length:\", num_encoder_tokens)\n",
        "        print(\"Tar voccab length:\", num_decoder_tokens)\n",
        "        print(\"Max iput sequence length:\", max_source_length)\n",
        "        print(\"Max output sequence length:\", max_target_length)\n",
        "\n",
        "        return self.encode(source_words, target_words, source_chars, target_chars)\n",
        "    \n",
        "    def encode(self, source, target, source_chars, target_chars, src_charTOint=None, tar_charTOint=None):\n",
        "        num_decoder_tokens = len(target_chars)\n",
        "        num_encoder_tokens = len(source_chars)\n",
        "        max_source_length = max([len(txt) for txt in source])\n",
        "        max_target_length = max([len(txt) for txt in target])\n",
        "\n",
        "        source_voccab, target_voccab = None, None\n",
        "        if src_charTOint == None and tar_charTOint == None:\n",
        "\n",
        "            print(\"Dictionary lookups for char to int mapping and vice versa\")\n",
        "            src_charTOint, src_intTOchar = self.dictionary_lookup(source_chars)\n",
        "            tar_charTOint, tar_intTOchar = self.dictionary_lookup(target_chars)\n",
        "\n",
        "            source_voccab = (src_charTOint, src_intTOchar)\n",
        "            target_voccab = (tar_charTOint, tar_intTOchar)\n",
        "\n",
        "        encoder_input_data = np.zeros(\n",
        "            (len(source), max_source_length, num_encoder_tokens), dtype=\"float32\"\n",
        "        )\n",
        "        decoder_input_data = np.zeros(\n",
        "            (len(source), max_target_length, num_decoder_tokens), dtype=\"float32\"\n",
        "        )\n",
        "        decoder_target_data = np.zeros(\n",
        "            (len(source), max_target_length, num_decoder_tokens), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        for i, (input_text, target_text) in enumerate(zip(source, target)):\n",
        "            for t, char in enumerate(input_text):\n",
        "                encoder_input_data[i, t, src_charTOint[char]] = 1.0\n",
        "            encoder_input_data[i, t + 1 :, src_charTOint[\" \"]] = 1.0\n",
        "            for t, char in enumerate(target_text):\n",
        "                \n",
        "                decoder_input_data[i, t, tar_charTOint[char]] = 1.0\n",
        "                if t > 0:\n",
        "                    decoder_target_data[i, t - 1, tar_charTOint[char]] = 1.0\n",
        "\n",
        "            decoder_input_data[i, t + 1 :, tar_charTOint[\" \"]] = 1.0\n",
        "            decoder_target_data[i, t:, tar_charTOint[\" \"]] = 1.0\n",
        "\n",
        "        if source_voccab != None and target_voccab != None:\n",
        "            return (\n",
        "                encoder_input_data,\n",
        "                decoder_input_data,\n",
        "                decoder_target_data,\n",
        "                source_voccab,\n",
        "                target_voccab,\n",
        "            )\n",
        "        else:\n",
        "\n",
        "            return encoder_input_data, decoder_input_data, decoder_target_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYb2pXHTZKc0"
      },
      "source": [
        "### 2.2 Processing the database\n",
        "\n",
        "Default input language is English and output language is Telugu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F-kjn6OdZVkz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No. of samples: 58550\n",
            "Src voccab length: 27\n",
            "Tar voccab length: 66\n",
            "Max iput sequence length: 25\n",
            "Max output sequence length: 22\n",
            "Dictionary lookups for char to int mapping and vice versa\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "DATAPATH = r\"dakshina_dataset_v1.0\"\n",
        "\n",
        "dataBase = DataProcessing(DATAPATH) \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lKyABHLuZqGx"
      },
      "source": [
        "## 3. RNN model for seq2seq machine translation \n",
        "### 3.1 Seq2Seq Translation Model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AT83-HQ3Z5fe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import layers\n",
        " \n",
        "\n",
        "#from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Dense, Input, InputLayer, Flatten, Activation, LSTM, SimpleRNN, GRU, TimeDistributed\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model, Sequential,  Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "\n",
        "class S2STranslation():\n",
        "\n",
        "    def __init__(self, modelConfigDict, srcChar2Int, tgtChar2Int, using_pretrained_model = False):\n",
        "        self.nEncoder = modelConfigDict[\"nEncoder\"]\n",
        "        self.cell_type = modelConfigDict[\"cell_type\"]\n",
        "        self.latentDim = modelConfigDict[\"latentDim\"]\n",
        "        self.dropout = modelConfigDict[\"dropout\"]\n",
        "        self.nDecoders = modelConfigDict[\"nDecoders\"]\n",
        "        self.hidden = modelConfigDict[\"hidden\"]\n",
        "        self.tgtChar2Int = tgtChar2Int\n",
        "        self.srcChar2Int = srcChar2Int\n",
        "\n",
        "    def build_configurable_model(self):   \n",
        "\n",
        "        #RNN    \n",
        "        if self.cell_type == \"RNN\":\n",
        "            # encoder\n",
        "            encoder_inputs = Input(shape=(None, len(self.srcChar2Int)))\n",
        "            encoder_outputs = encoder_inputs\n",
        "            for i in range(1, self.nEncoder + 1):\n",
        "                encoder = SimpleRNN(\n",
        "                    self.latentDim,\n",
        "                    return_state=True,\n",
        "                    return_sequences=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                encoder_outputs, state = encoder(encoder_inputs)\n",
        "            encoder_states = [state]\n",
        "\n",
        "            # decoder\n",
        "            decoder_inputs = Input(shape=(None, len(self.tgtChar2Int)))\n",
        "            decoder_outputs = decoder_inputs\n",
        "            for i in range(1, self.nDecoders + 1):\n",
        "                decoder = SimpleRNN(\n",
        "                    self.latentDim,\n",
        "                    return_sequences=True,\n",
        "                    return_state=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                decoder_outputs, _ = decoder(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "            # dense\n",
        "            hidden = Dense(self.hidden, activation=\"relu\")\n",
        "            hidden_outputs = hidden(decoder_outputs)\n",
        "            decoder_dense = Dense(len(self.tgtChar2Int), activation=\"softmax\")\n",
        "            decoder_outputs = decoder_dense(hidden_outputs)\n",
        "            model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "            \n",
        "            return model\n",
        "\n",
        "        #LSTM\n",
        "        elif self.cell_type == \"LSTM\":\n",
        "\n",
        "            # encoder\n",
        "            encoder_inputs = Input(shape=(None, len(self.srcChar2Int)))\n",
        "            encoder_outputs = encoder_inputs\n",
        "            for i in range(1, self.nEncoder + 1):\n",
        "                encoder = LSTM(\n",
        "                    self.latentDim,\n",
        "                    return_state=True,\n",
        "                    return_sequences=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                encoder_outputs, state_h, state_c = encoder(encoder_outputs)\n",
        "            encoder_states = [state_h, state_c]\n",
        "\n",
        "            # decoder\n",
        "            decoder_inputs = Input(shape=(None, len(self.tgtChar2Int)))\n",
        "            decoder_outputs = decoder_inputs\n",
        "            for i in range(1, self.nDecoders + 1):\n",
        "                decoder = LSTM(\n",
        "                    self.latentDim,\n",
        "                    return_state=True,\n",
        "                    return_sequences=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                decoder_outputs, _, _ = decoder(\n",
        "                    decoder_outputs, initial_state=encoder_states\n",
        "                )\n",
        "\n",
        "            # dense\n",
        "            hidden = Dense(self.hidden, activation=\"relu\")\n",
        "            hidden_outputs = hidden(decoder_outputs)\n",
        "            decoder_dense = Dense(len(self.tgtChar2Int), activation=\"softmax\")\n",
        "            decoder_outputs = decoder_dense(hidden_outputs)\n",
        "            model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "            \n",
        "            return model\n",
        "        #GRU\n",
        "        elif self.cell_type == \"GRU\":\n",
        "\n",
        "            # encoder\n",
        "            encoder_inputs = Input(shape=(None, len(self.srcChar2Int)))\n",
        "            encoder_outputs = encoder_inputs\n",
        "            for i in range(1, self.nEncoder + 1):\n",
        "                encoder = GRU(\n",
        "                    self.latentDim,\n",
        "                    return_state=True,\n",
        "                    return_sequences=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                encoder_outputs, state = encoder(encoder_inputs)\n",
        "            encoder_states = [state]\n",
        "\n",
        "            # decoder\n",
        "            decoder_inputs = Input(shape=(None, len(self.tgtChar2Int)))\n",
        "            decoder_outputs = decoder_inputs\n",
        "            for i in range(1, self.nDecoders + 1):\n",
        "                decoder = GRU(\n",
        "                    self.latentDim,\n",
        "                    return_sequences=True,\n",
        "                    return_state=True,\n",
        "                    dropout=self.dropout,\n",
        "                )\n",
        "                decoder_outputs, _ = decoder(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "            # dense\n",
        "            hidden = Dense(self.hidden, activation=\"relu\")\n",
        "            hidden_outputs = hidden(decoder_outputs)\n",
        "            decoder_dense = Dense(len(self.tgtChar2Int), activation=\"softmax\")\n",
        "            decoder_outputs = decoder_dense(hidden_outputs)\n",
        "            model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "            \n",
        "            return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjLa_0loaBPT"
      },
      "source": [
        "### 3.2 Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F-B1RkORaHjO"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import RNN, LSTM, GRU, Dense\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "#using a gpu\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "try:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "def train():\n",
        "    default_config = {\n",
        "        \"cell_type\": \"RNN\",\n",
        "        \"latentDim\": 256,\n",
        "        \"hidden\": 128,\n",
        "        \"optimiser\": \"rmsprop\",\n",
        "        \"numEncoders\": 1,\n",
        "        \"numDecoders\": 1,\n",
        "        \"dropout\": 0.2,\n",
        "        \"epochs\": 1,\n",
        "        \"batch_size\": 64,\n",
        "    }\n",
        "\n",
        "    wandb.init(config=default_config)\n",
        "    wandb.init(config=default_config,  project=\"Assignment-3_WithoutAttention\", entity=\"karanwxlia\")\n",
        "    config = wandb.config\n",
        "    \n",
        "    wandb.run.name = (\n",
        "        str(config.cell_type)\n",
        "        + dataBase.source_lang\n",
        "        + str(config.numEncoders)\n",
        "        + \"_\"\n",
        "        + dataBase.target_lang\n",
        "        + \"_\"\n",
        "        + str(config.numDecoders)\n",
        "        + \"_\"\n",
        "        + config.optimiser\n",
        "        + \"_\"\n",
        "        + str(config.epochs)\n",
        "        + \"_\"\n",
        "        + str(config.dropout) \n",
        "        + \"_\"\n",
        "        + str(config.batch_size)\n",
        "        + \"_\"\n",
        "        + str(config.latentDim)\n",
        "    )\n",
        "    wandb.run.save()\n",
        "\n",
        "    modelInit = S2STranslation(config,srcChar2Int=dataBase.src_charTOint, tgtChar2Int=dataBase.tar_charTOint)\n",
        "    \n",
        "    model = modelInit.build_attention_model()\n",
        "    model.summary()\n",
        "    model.compile(\n",
        "        optimizer=config.optimiser,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    earlystopping = EarlyStopping(\n",
        "        monitor=\"val_accuracy\", min_delta=0.01, patience=5, verbose=2, mode=\"auto\"\n",
        "    )\n",
        "    model.fit(\n",
        "        [dataBase.train_encoder_input, dataBase.train_decoder_input],\n",
        "        dataBase.train_decoder_target,\n",
        "        batch_size=config.batch_size,\n",
        "        epochs=config.epochs,\n",
        "        validation_data=([dataBase.val_encoder_input, dataBase.val_decoder_input], dataBase.val_decoder_target),\n",
        "        callbacks=[earlystopping, WandbCallback()],\n",
        "    )\n",
        "\n",
        "    model.save(os.path.join(\"./TrainedAttentionModels\", wandb.run.name))    \n",
        "    wandb.finish()\n",
        "    \n",
        "    #return model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwIBB2t3ahQK"
      },
      "source": [
        "Running the wandb sweep: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hbZXjvV9agMF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: vn4o92i4\n",
            "Sweep URL: https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xsqmjfi5 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatentDim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumDecoders: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumEncoders: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: adam\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkaranwxlia\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">fragrant-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/xsqmjfi5\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/xsqmjfi5</a><br/>\n",
              "                Run data is saved locally in <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_185137-xsqmjfi5</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 27)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, None, 256),  290816      ['input_1[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None, 66)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  525312      ['lstm[0][0]']                   \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, None, 256),  330752      ['input_2[0][0]',                \n",
            "                                 (None, 256),                     'lstm_1[0][1]',                 \n",
            "                                 (None, 256)]                     'lstm_1[0][2]']                 \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 128)    32896       ['lstm_2[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 66)     8514        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,188,290\n",
            "Trainable params: 1,188,290\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C541571FC0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C541571FC0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "915/915 [==============================] - ETA: 0s - loss: 1.1830 - accuracy: 0.6829WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C543123BE0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C543123BE0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "915/915 [==============================] - 248s 254ms/step - loss: 1.1830 - accuracy: 0.6829 - val_loss: 1.5810 - val_accuracy: 0.6918\n",
            "Epoch 2/5\n",
            "915/915 [==============================] - 233s 255ms/step - loss: 0.8224 - accuracy: 0.7663 - val_loss: 1.4754 - val_accuracy: 0.7352\n",
            "Epoch 3/5\n",
            "915/915 [==============================] - 241s 264ms/step - loss: 0.5628 - accuracy: 0.8371 - val_loss: 1.5548 - val_accuracy: 0.7478\n",
            "Epoch 4/5\n",
            "915/915 [==============================] - 230s 251ms/step - loss: 0.4581 - accuracy: 0.8658 - val_loss: 1.5472 - val_accuracy: 0.7634\n",
            "Epoch 5/5\n",
            "915/915 [==============================] - 243s 265ms/step - loss: 0.3999 - accuracy: 0.8823 - val_loss: 1.6130 - val_accuracy: 0.7556\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\LSTMen2_te_1_adam_5_0.2_64_256\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\LSTMen2_te_1_adam_5_0.2_64_256\\assets\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 24056<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_185137-xsqmjfi5\\logs\\debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_185137-xsqmjfi5\\logs\\debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>0.88233</td></tr><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>1.47542</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.39994</td></tr><tr><td>val_accuracy</td><td>0.75557</td></tr><tr><td>val_loss</td><td>1.61298</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>▁▄▆▇█</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▅▂▂▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆█▇</td></tr><tr><td>val_loss</td><td>▆▁▅▅█</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">fragrant-sweep-1</strong>: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/xsqmjfi5\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/xsqmjfi5</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: brztztyc with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatentDim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumDecoders: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumEncoders: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: rmsprop\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">sparkling-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/brztztyc\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/brztztyc</a><br/>\n",
              "                Run data is saved locally in <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_191225-brztztyc</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 27)]   0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None, 66)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, None, 256),  290816      ['input_1[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  330752      ['input_2[0][0]',                \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, None, 256),  525312      ['lstm_1[0][0]',                 \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 256),  525312      ['lstm_2[0][0]',                 \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 64)     16448       ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 66)     4290        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,692,930\n",
            "Trainable params: 1,692,930\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C553811480> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C553811480> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C553811480> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1830/1830 [==============================] - ETA: 0s - loss: 1.0710 - accuracy: 0.7054WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C5459088B0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C5459088B0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C5459088B0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1830/1830 [==============================] - 312s 168ms/step - loss: 1.0710 - accuracy: 0.7054 - val_loss: 1.6605 - val_accuracy: 0.6924\n",
            "Epoch 2/5\n",
            "1830/1830 [==============================] - 276s 151ms/step - loss: 0.7341 - accuracy: 0.7891 - val_loss: 1.8384 - val_accuracy: 0.7016\n",
            "Epoch 3/5\n",
            "1830/1830 [==============================] - 268s 146ms/step - loss: 0.5540 - accuracy: 0.8383 - val_loss: 1.7951 - val_accuracy: 0.7302\n",
            "Epoch 4/5\n",
            "1830/1830 [==============================] - 276s 151ms/step - loss: 0.4443 - accuracy: 0.8693 - val_loss: 1.7918 - val_accuracy: 0.7395\n",
            "Epoch 5/5\n",
            "1830/1830 [==============================] - 256s 140ms/step - loss: 0.3834 - accuracy: 0.8867 - val_loss: 1.8642 - val_accuracy: 0.7577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\LSTMen1_te_3_rmsprop_5_0.2_32_256\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\LSTMen1_te_3_rmsprop_5_0.2_32_256\\assets\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 10084<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_191225-brztztyc\\logs\\debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_191225-brztztyc\\logs\\debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>0.88666</td></tr><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>1.66054</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.38339</td></tr><tr><td>val_accuracy</td><td>0.75773</td></tr><tr><td>val_loss</td><td>1.86419</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>▁▄▆▇█</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▅▃▂▁</td></tr><tr><td>val_accuracy</td><td>▁▂▅▆█</td></tr><tr><td>val_loss</td><td>▁▇▆▆█</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">sparkling-sweep-2</strong>: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/brztztyc\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/brztztyc</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pvru7t8h with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatentDim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumDecoders: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumEncoders: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: rmsprop\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">stellar-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/pvru7t8h\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/pvru7t8h</a><br/>\n",
              "                Run data is saved locally in <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_193559-pvru7t8h</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 27)]   0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None, 66)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, None, 256),  290816      ['input_1[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  330752      ['input_2[0][0]',                \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, None, 256),  525312      ['lstm_1[0][0]',                 \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 256),  525312      ['lstm_2[0][0]',                 \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 128)    32896       ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 66)     8514        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,713,602\n",
            "Trainable params: 1,713,602\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C55A32A440> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C55A32A440> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C55A32A440> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "915/915 [==============================] - ETA: 0s - loss: 1.1615 - accuracy: 0.6843WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C56C9FB910> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C56C9FB910> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C56C9FB910> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "915/915 [==============================] - 177s 188ms/step - loss: 1.1615 - accuracy: 0.6843 - val_loss: 1.4285 - val_accuracy: 0.6992\n",
            "Epoch 2/20\n",
            "915/915 [==============================] - 182s 199ms/step - loss: 0.8292 - accuracy: 0.7636 - val_loss: 1.7564 - val_accuracy: 0.7000\n",
            "Epoch 3/20\n",
            "915/915 [==============================] - 192s 210ms/step - loss: 0.6722 - accuracy: 0.8048 - val_loss: 1.7506 - val_accuracy: 0.7180\n",
            "Epoch 4/20\n",
            "915/915 [==============================] - 182s 199ms/step - loss: 0.5556 - accuracy: 0.8368 - val_loss: 1.7366 - val_accuracy: 0.7398\n",
            "Epoch 5/20\n",
            "915/915 [==============================] - 192s 210ms/step - loss: 0.4700 - accuracy: 0.8606 - val_loss: 1.6836 - val_accuracy: 0.7426\n",
            "Epoch 6/20\n",
            "915/915 [==============================] - 181s 198ms/step - loss: 0.4112 - accuracy: 0.8772 - val_loss: 1.8482 - val_accuracy: 0.7453\n",
            "Epoch 7/20\n",
            "915/915 [==============================] - 181s 197ms/step - loss: 0.3696 - accuracy: 0.8893 - val_loss: 1.8138 - val_accuracy: 0.7471\n",
            "Epoch 8/20\n",
            "915/915 [==============================] - 201s 219ms/step - loss: 0.3390 - accuracy: 0.8980 - val_loss: 1.8236 - val_accuracy: 0.7582\n",
            "Epoch 9/20\n",
            "915/915 [==============================] - 188s 205ms/step - loss: 0.3144 - accuracy: 0.9051 - val_loss: 1.7588 - val_accuracy: 0.7641\n",
            "Epoch 10/20\n",
            "915/915 [==============================] - 196s 214ms/step - loss: 0.2951 - accuracy: 0.9104 - val_loss: 1.8498 - val_accuracy: 0.7598\n",
            "Epoch 11/20\n",
            "915/915 [==============================] - 199s 217ms/step - loss: 0.2806 - accuracy: 0.9146 - val_loss: 1.8620 - val_accuracy: 0.7630\n",
            "Epoch 12/20\n",
            "915/915 [==============================] - 195s 213ms/step - loss: 0.2660 - accuracy: 0.9192 - val_loss: 2.0439 - val_accuracy: 0.7520\n",
            "Epoch 13/20\n",
            "915/915 [==============================] - 193s 211ms/step - loss: 0.2553 - accuracy: 0.9220 - val_loss: 1.9237 - val_accuracy: 0.7687\n",
            "Epoch 14/20\n",
            "915/915 [==============================] - 186s 204ms/step - loss: 0.2450 - accuracy: 0.9250 - val_loss: 1.9557 - val_accuracy: 0.7586\n",
            "Epoch 15/20\n",
            "915/915 [==============================] - 189s 207ms/step - loss: 0.2366 - accuracy: 0.9275 - val_loss: 1.9238 - val_accuracy: 0.7694\n",
            "Epoch 16/20\n",
            "915/915 [==============================] - 178s 195ms/step - loss: 0.2306 - accuracy: 0.9293 - val_loss: 1.9336 - val_accuracy: 0.7594\n",
            "Epoch 17/20\n",
            "915/915 [==============================] - 184s 201ms/step - loss: 0.2220 - accuracy: 0.9318 - val_loss: 1.9797 - val_accuracy: 0.7678\n",
            "Epoch 18/20\n",
            "915/915 [==============================] - 171s 187ms/step - loss: 0.2158 - accuracy: 0.9336 - val_loss: 1.9127 - val_accuracy: 0.7778\n",
            "Epoch 18: early stopping\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\LSTMen1_te_3_rmsprop_20_0.2_64_256\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\LSTMen1_te_3_rmsprop_20_0.2_64_256\\assets\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 17920<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_193559-pvru7t8h\\logs\\debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_193559-pvru7t8h\\logs\\debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>0.93357</td></tr><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>1.42848</td></tr><tr><td>epoch</td><td>17</td></tr><tr><td>loss</td><td>0.21581</td></tr><tr><td>val_accuracy</td><td>0.77782</td></tr><tr><td>val_loss</td><td>1.9127</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>▁▃▄▅▆▆▇▇▇▇▇███████</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>loss</td><td>█▆▄▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▃▅▅▅▅▆▇▆▇▆▇▆▇▆▇█</td></tr><tr><td>val_loss</td><td>▁▅▅▅▄▆▅▅▅▆▆█▇▇▇▇▇▇</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">stellar-sweep-3</strong>: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/pvru7t8h\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/pvru7t8h</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: elnabywc with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatentDim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumDecoders: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumEncoders: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: adam\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">bright-sweep-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/elnabywc\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/elnabywc</a><br/>\n",
              "                Run data is saved locally in <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_203228-elnabywc</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 27)]   0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None, 66)]   0           []                               \n",
            "                                                                                                  \n",
            " simple_rnn_1 (SimpleRNN)       [(None, None, 256),  72704       ['input_1[0][0]']                \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " simple_rnn_3 (SimpleRNN)       [(None, None, 256),  82688       ['input_2[0][0]',                \n",
            "                                 (None, 256)]                     'simple_rnn_1[0][1]']           \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 64)     16448       ['simple_rnn_3[0][0]']           \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 66)     4290        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 176,130\n",
            "Trainable params: 176,130\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C543123880> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C543123880> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C543123880> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1830/1830 [==============================] - ETA: 0s - loss: 0.8947 - accuracy: 0.7556WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C543120F70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C543120F70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C543120F70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1830/1830 [==============================] - 26s 14ms/step - loss: 0.8947 - accuracy: 0.7556 - val_loss: 1.9214 - val_accuracy: 0.6400\n",
            "Epoch 2/5\n",
            "1830/1830 [==============================] - 25s 14ms/step - loss: 0.6481 - accuracy: 0.8148 - val_loss: 2.1770 - val_accuracy: 0.6326\n",
            "Epoch 3/5\n",
            "1830/1830 [==============================] - 25s 14ms/step - loss: 0.5903 - accuracy: 0.8298 - val_loss: 1.9548 - val_accuracy: 0.6565\n",
            "Epoch 4/5\n",
            "1830/1830 [==============================] - 25s 14ms/step - loss: 0.5559 - accuracy: 0.8392 - val_loss: 2.1248 - val_accuracy: 0.6508\n",
            "Epoch 5/5\n",
            "1830/1830 [==============================] - 25s 14ms/step - loss: 0.5315 - accuracy: 0.8453 - val_loss: 2.2441 - val_accuracy: 0.6465\n",
            "INFO:tensorflow:Assets written to: ./TrainedModels\\RNNen2_te_2_adam_5_0.1_32_256\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\RNNen2_te_2_adam_5_0.1_32_256\\assets\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 26648<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_203228-elnabywc\\logs\\debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_203228-elnabywc\\logs\\debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>0.8453</td></tr><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>1.92136</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.53152</td></tr><tr><td>val_accuracy</td><td>0.64646</td></tr><tr><td>val_loss</td><td>2.24412</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>▁▆▇██</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▃▂▁▁</td></tr><tr><td>val_accuracy</td><td>▃▁█▆▅</td></tr><tr><td>val_loss</td><td>▁▇▂▅█</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">bright-sweep-4</strong>: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/elnabywc\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/elnabywc</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5sdjmp52 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatentDim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumDecoders: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumEncoders: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: rmsprop\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">fast-sweep-5</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/5sdjmp52\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/5sdjmp52</a><br/>\n",
              "                Run data is saved locally in <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_203449-5sdjmp52</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 27)]   0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None, 66)]   0           []                               \n",
            "                                                                                                  \n",
            " simple_rnn (SimpleRNN)         [(None, None, 256),  72704       ['input_1[0][0]']                \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " simple_rnn_1 (SimpleRNN)       [(None, None, 256),  82688       ['input_2[0][0]',                \n",
            "                                 (None, 256)]                     'simple_rnn[0][1]']             \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 128)    32896       ['simple_rnn_1[0][0]']           \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 66)     8514        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 196,802\n",
            "Trainable params: 196,802\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C54F823520> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C54F823520> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C54F823520> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1829/1830 [============================>.] - ETA: 0s - loss: 1.1321 - accuracy: 0.6897WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C54F821EA0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C54F821EA0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C54F821EA0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1830/1830 [==============================] - 27s 14ms/step - loss: 1.1321 - accuracy: 0.6897 - val_loss: 1.3783 - val_accuracy: 0.6890\n",
            "Epoch 2/20\n",
            "1830/1830 [==============================] - 26s 14ms/step - loss: 1.0380 - accuracy: 0.7090 - val_loss: 1.4238 - val_accuracy: 0.6925\n",
            "Epoch 3/20\n",
            "1830/1830 [==============================] - 26s 14ms/step - loss: 1.0121 - accuracy: 0.7157 - val_loss: 1.4466 - val_accuracy: 0.6886\n",
            "Epoch 4/20\n",
            "1830/1830 [==============================] - 26s 14ms/step - loss: 0.9943 - accuracy: 0.7196 - val_loss: 1.4399 - val_accuracy: 0.6898\n",
            "Epoch 5/20\n",
            "1830/1830 [==============================] - 26s 14ms/step - loss: 0.9855 - accuracy: 0.7217 - val_loss: 1.4341 - val_accuracy: 0.6898\n",
            "Epoch 6/20\n",
            "1830/1830 [==============================] - 26s 14ms/step - loss: 0.9800 - accuracy: 0.7232 - val_loss: 1.4827 - val_accuracy: 0.6929\n",
            "Epoch 6: early stopping\n",
            "INFO:tensorflow:Assets written to: ./TrainedModels\\RNNen1_te_1_rmsprop_20_0.3_32_256\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\RNNen1_te_1_rmsprop_20_0.3_32_256\\assets\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 30484<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_203449-5sdjmp52\\logs\\debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_203449-5sdjmp52\\logs\\debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>0.72321</td></tr><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>1.37828</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>loss</td><td>0.98003</td></tr><tr><td>val_accuracy</td><td>0.69285</td></tr><tr><td>val_loss</td><td>1.48269</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>▁▅▆▇██</td></tr><tr><td>epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>loss</td><td>█▄▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▂▇▁▃▃█</td></tr><tr><td>val_loss</td><td>▁▄▆▅▅█</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">fast-sweep-5</strong>: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/5sdjmp52\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/5sdjmp52</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 39gbvht0 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatentDim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumDecoders: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumEncoders: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: rmsprop\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">good-sweep-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/39gbvht0\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/39gbvht0</a><br/>\n",
              "                Run data is saved locally in <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_203740-39gbvht0</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 27)]   0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None, 66)]   0           []                               \n",
            "                                                                                                  \n",
            " simple_rnn_2 (SimpleRNN)       [(None, None, 256),  72704       ['input_1[0][0]']                \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " simple_rnn_4 (SimpleRNN)       [(None, None, 256),  82688       ['input_2[0][0]',                \n",
            "                                 (None, 256)]                     'simple_rnn_2[0][1]']           \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 64)     16448       ['simple_rnn_4[0][0]']           \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 66)     4290        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 176,130\n",
            "Trainable params: 176,130\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C57291DA20> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C57291DA20> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C57291DA20> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1830/1830 [==============================] - ETA: 0s - loss: 1.1550 - accuracy: 0.6855WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C553812950> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C553812950> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C553812950> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1830/1830 [==============================] - 27s 14ms/step - loss: 1.1550 - accuracy: 0.6855 - val_loss: 1.4072 - val_accuracy: 0.6863\n",
            "Epoch 2/5\n",
            "1830/1830 [==============================] - 26s 14ms/step - loss: 1.0538 - accuracy: 0.7059 - val_loss: 1.4296 - val_accuracy: 0.6907\n",
            "Epoch 3/5\n",
            "1830/1830 [==============================] - 26s 14ms/step - loss: 1.0308 - accuracy: 0.7116 - val_loss: 1.4233 - val_accuracy: 0.6927\n",
            "Epoch 4/5\n",
            "1830/1830 [==============================] - 26s 14ms/step - loss: 1.0033 - accuracy: 0.7166 - val_loss: 1.6038 - val_accuracy: 0.6939\n",
            "Epoch 5/5\n",
            "1830/1830 [==============================] - 26s 14ms/step - loss: 0.9919 - accuracy: 0.7197 - val_loss: 1.5252 - val_accuracy: 0.6947\n",
            "INFO:tensorflow:Assets written to: ./TrainedModels\\RNNen3_te_2_rmsprop_5_0.3_32_256\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\RNNen3_te_2_rmsprop_5_0.3_32_256\\assets\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 16772<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_203740-39gbvht0\\logs\\debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_203740-39gbvht0\\logs\\debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>0.71966</td></tr><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>1.40719</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.99188</td></tr><tr><td>val_accuracy</td><td>0.69475</td></tr><tr><td>val_loss</td><td>1.52515</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>▁▅▆▇█</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▄▃▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇█</td></tr><tr><td>val_loss</td><td>▁▂▂█▅</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">good-sweep-6</strong>: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/39gbvht0\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/39gbvht0</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uy46he9k with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatentDim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumDecoders: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumEncoders: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: adam\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">floral-sweep-7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/uy46he9k\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/uy46he9k</a><br/>\n",
              "                Run data is saved locally in <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_204003-uy46he9k</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 27)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, None, 256),  290816      ['input_1[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None, 66)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  525312      ['lstm[0][0]']                   \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, None, 256),  330752      ['input_2[0][0]',                \n",
            "                                 (None, 256),                     'lstm_1[0][1]',                 \n",
            "                                 (None, 256)]                     'lstm_1[0][2]']                 \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 256),  525312      ['lstm_2[0][0]',                 \n",
            "                                 (None, 256),                     'lstm_1[0][1]',                 \n",
            "                                 (None, 256)]                     'lstm_1[0][2]']                 \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 128)    32896       ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 66)     8514        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,713,602\n",
            "Trainable params: 1,713,602\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C57291F520> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C57291F520> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C57291F520> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "915/915 [==============================] - ETA: 0s - loss: 1.1958 - accuracy: 0.6753WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C543120C10> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C543120C10> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C543120C10> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "915/915 [==============================] - 182s 194ms/step - loss: 1.1958 - accuracy: 0.6753 - val_loss: 1.5846 - val_accuracy: 0.6828\n",
            "Epoch 2/5\n",
            "915/915 [==============================] - 183s 200ms/step - loss: 0.8853 - accuracy: 0.7500 - val_loss: 1.5682 - val_accuracy: 0.7130\n",
            "Epoch 3/5\n",
            "915/915 [==============================] - 185s 202ms/step - loss: 0.6603 - accuracy: 0.8099 - val_loss: 1.6180 - val_accuracy: 0.7191\n",
            "Epoch 4/5\n",
            "915/915 [==============================] - 175s 192ms/step - loss: 0.5546 - accuracy: 0.8380 - val_loss: 1.6720 - val_accuracy: 0.7299\n",
            "Epoch 5/5\n",
            "915/915 [==============================] - 172s 188ms/step - loss: 0.4962 - accuracy: 0.8536 - val_loss: 1.7474 - val_accuracy: 0.7281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\LSTMen2_te_2_adam_5_0.3_64_256\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\LSTMen2_te_2_adam_5_0.3_64_256\\assets\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 28208<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_204003-uy46he9k\\logs\\debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_204003-uy46he9k\\logs\\debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>0.85358</td></tr><tr><td>best_epoch</td><td>1</td></tr><tr><td>best_val_loss</td><td>1.56818</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.49625</td></tr><tr><td>val_accuracy</td><td>0.72806</td></tr><tr><td>val_loss</td><td>1.74741</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>▁▄▆▇█</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▅▃▂▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆██</td></tr><tr><td>val_loss</td><td>▂▁▃▅█</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">floral-sweep-7</strong>: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/uy46he9k\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/uy46he9k</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4jrynx0b with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatentDim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumDecoders: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumEncoders: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: rmsprop\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">easy-sweep-8</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/4jrynx0b\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/4jrynx0b</a><br/>\n",
              "                Run data is saved locally in <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_205527-4jrynx0b</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 27)]   0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None, 66)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, None, 256),  290816      ['input_1[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  330752      ['input_2[0][0]',                \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, None, 256),  525312      ['lstm_1[0][0]',                 \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 256),  525312      ['lstm_2[0][0]',                 \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 64)     16448       ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 66)     4290        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,692,930\n",
            "Trainable params: 1,692,930\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C57291FA30> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C57291FA30> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C57291FA30> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "915/915 [==============================] - ETA: 0s - loss: 1.2058 - accuracy: 0.6743WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C56C9FBEB0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C56C9FBEB0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C56C9FBEB0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "915/915 [==============================] - 160s 169ms/step - loss: 1.2058 - accuracy: 0.6743 - val_loss: 1.4836 - val_accuracy: 0.6873\n",
            "Epoch 2/15\n",
            "915/915 [==============================] - 181s 198ms/step - loss: 0.9047 - accuracy: 0.7452 - val_loss: 1.7432 - val_accuracy: 0.6998\n",
            "Epoch 3/15\n",
            "915/915 [==============================] - 179s 196ms/step - loss: 0.7635 - accuracy: 0.7814 - val_loss: 1.8015 - val_accuracy: 0.7050\n",
            "Epoch 4/15\n",
            "915/915 [==============================] - 154s 169ms/step - loss: 0.6553 - accuracy: 0.8101 - val_loss: 1.7294 - val_accuracy: 0.7202\n",
            "Epoch 5/15\n",
            "915/915 [==============================] - 167s 182ms/step - loss: 0.5765 - accuracy: 0.8317 - val_loss: 1.7608 - val_accuracy: 0.7251\n",
            "Epoch 6/15\n",
            "915/915 [==============================] - 178s 195ms/step - loss: 0.5212 - accuracy: 0.8470 - val_loss: 1.7645 - val_accuracy: 0.7352\n",
            "Epoch 7/15\n",
            "915/915 [==============================] - 180s 196ms/step - loss: 0.4835 - accuracy: 0.8576 - val_loss: 1.8206 - val_accuracy: 0.7410\n",
            "Epoch 8/15\n",
            "915/915 [==============================] - 185s 203ms/step - loss: 0.4548 - accuracy: 0.8649 - val_loss: 1.8346 - val_accuracy: 0.7454\n",
            "Epoch 9/15\n",
            "915/915 [==============================] - 179s 196ms/step - loss: 0.4308 - accuracy: 0.8721 - val_loss: 1.7352 - val_accuracy: 0.7550\n",
            "Epoch 10/15\n",
            "915/915 [==============================] - 190s 208ms/step - loss: 0.4133 - accuracy: 0.8768 - val_loss: 1.9012 - val_accuracy: 0.7526\n",
            "Epoch 11/15\n",
            "915/915 [==============================] - 178s 195ms/step - loss: 0.3973 - accuracy: 0.8814 - val_loss: 1.8479 - val_accuracy: 0.7471\n",
            "Epoch 12/15\n",
            "915/915 [==============================] - 171s 187ms/step - loss: 0.3856 - accuracy: 0.8845 - val_loss: 1.8524 - val_accuracy: 0.7584\n",
            "Epoch 13/15\n",
            "915/915 [==============================] - 183s 200ms/step - loss: 0.3741 - accuracy: 0.8880 - val_loss: 1.7921 - val_accuracy: 0.7637\n",
            "Epoch 14/15\n",
            "915/915 [==============================] - 187s 204ms/step - loss: 0.3616 - accuracy: 0.8913 - val_loss: 1.8074 - val_accuracy: 0.7650\n",
            "Epoch 15/15\n",
            "915/915 [==============================] - 208s 228ms/step - loss: 0.3530 - accuracy: 0.8936 - val_loss: 1.8013 - val_accuracy: 0.7670\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\LSTMen1_te_3_rmsprop_15_0.3_64_256\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\LSTMen1_te_3_rmsprop_15_0.3_64_256\\assets\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 22284<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_205527-4jrynx0b\\logs\\debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_205527-4jrynx0b\\logs\\debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>0.89361</td></tr><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>1.48364</td></tr><tr><td>epoch</td><td>14</td></tr><tr><td>loss</td><td>0.35303</td></tr><tr><td>val_accuracy</td><td>0.76696</td></tr><tr><td>val_loss</td><td>1.80127</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>accuracy</td><td>▁▃▄▅▆▇▇▇▇▇█████</td></tr><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>loss</td><td>█▆▄▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▃▄▄▅▆▆▇▇▆▇███</td></tr><tr><td>val_loss</td><td>▁▅▆▅▆▆▇▇▅█▇▇▆▆▆</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">easy-sweep-8</strong>: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/4jrynx0b\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/4jrynx0b</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g1w1xo66 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatentDim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumDecoders: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumEncoders: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: rmsprop\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">dandy-sweep-9</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/sweeps/vn4o92i4</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/g1w1xo66\" target=\"_blank\">https://wandb.ai/karanwxlia/Assignment-3_WithoutAttention/runs/g1w1xo66</a><br/>\n",
              "                Run data is saved locally in <code>c:\\Users\\KARAN\\Desktop\\Assignmets for cs6910\\wandb\\run-20230128_214036-g1w1xo66</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 27)]   0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None, 66)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, None, 256),  290816      ['input_1[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  330752      ['input_2[0][0]',                \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, None, 256),  525312      ['lstm_1[0][0]',                 \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 128)    32896       ['lstm_2[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 66)     8514        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,188,290\n",
            "Trainable params: 1,188,290\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C56CF1D2D0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C56CF1D2D0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002C56CF1D2D0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "915/915 [==============================] - ETA: 0s - loss: 1.1653 - accuracy: 0.6827WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C54FB95F30> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C54FB95F30> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002C54FB95F30> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "915/915 [==============================] - 114s 120ms/step - loss: 1.1653 - accuracy: 0.6827 - val_loss: 1.5201 - val_accuracy: 0.6961\n",
            "Epoch 2/20\n",
            "915/915 [==============================] - 107s 117ms/step - loss: 0.8790 - accuracy: 0.7496 - val_loss: 1.7053 - val_accuracy: 0.7126\n",
            "Epoch 3/20\n",
            "915/915 [==============================] - 103s 112ms/step - loss: 0.6962 - accuracy: 0.7988 - val_loss: 1.6767 - val_accuracy: 0.7396\n",
            "Epoch 4/20\n",
            "915/915 [==============================] - 122s 133ms/step - loss: 0.5392 - accuracy: 0.8409 - val_loss: 1.7534 - val_accuracy: 0.7610\n",
            "Epoch 5/20\n",
            "915/915 [==============================] - 132s 145ms/step - loss: 0.4472 - accuracy: 0.8669 - val_loss: 1.8295 - val_accuracy: 0.7637\n",
            "Epoch 6/20\n",
            "915/915 [==============================] - 120s 131ms/step - loss: 0.3926 - accuracy: 0.8827 - val_loss: 1.8021 - val_accuracy: 0.7690\n",
            "Epoch 7/20\n",
            "915/915 [==============================] - 114s 124ms/step - loss: 0.3550 - accuracy: 0.8932 - val_loss: 1.9321 - val_accuracy: 0.7637\n",
            "Epoch 8/20\n",
            "915/915 [==============================] - 115s 126ms/step - loss: 0.3282 - accuracy: 0.9010 - val_loss: 1.9590 - val_accuracy: 0.7727\n",
            "Epoch 9/20\n",
            "915/915 [==============================] - 105s 115ms/step - loss: 0.3054 - accuracy: 0.9077 - val_loss: 2.0627 - val_accuracy: 0.7649\n",
            "Epoch 10/20\n",
            "915/915 [==============================] - 106s 116ms/step - loss: 0.2887 - accuracy: 0.9124 - val_loss: 2.0768 - val_accuracy: 0.7670\n",
            "Epoch 11/20\n",
            "915/915 [==============================] - 104s 114ms/step - loss: 0.2739 - accuracy: 0.9169 - val_loss: 2.0592 - val_accuracy: 0.7698\n",
            "Epoch 12/20\n",
            "915/915 [==============================] - 106s 115ms/step - loss: 0.2622 - accuracy: 0.9204 - val_loss: 2.1231 - val_accuracy: 0.7647\n",
            "Epoch 13/20\n",
            "915/915 [==============================] - 106s 115ms/step - loss: 0.2505 - accuracy: 0.9237 - val_loss: 2.1280 - val_accuracy: 0.7678\n",
            "Epoch 13: early stopping\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\LSTMen1_te_2_rmsprop_20_0.2_64_256\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./TrainedModels\\LSTMen1_te_2_rmsprop_20_0.2_64_256\\assets\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 24976<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "  \n",
        "sweep_config = {\n",
        "    \"name\": \"Bayesian Sweep without attention\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \n",
        "        \"cell_type\": {\"values\": [\"RNN\", \"GRU\", \"LSTM\"]},\n",
        "        \n",
        "        \"latentDim\": {\"values\": [256]},\n",
        "        \n",
        "        \"hidden\": {\"values\": [128, 64]},\n",
        "        \n",
        "        \"optimiser\": {\"values\": [\"rmsprop\", \"adam\"]},\n",
        "        \n",
        "        \"numEncoders\": {\"values\": [1, 2, 3]},\n",
        "        \n",
        "        \"numDecoders\": {\"values\": [1, 2, 3]},\n",
        "        \n",
        "        \"dropout\": {\"values\": [0.1, 0.2, 0.3]},\n",
        "        \n",
        "        \"epochs\": {\"values\": [5,10,15, 20]},\n",
        "        \n",
        "        \"batch_size\": {\"values\": [32, 64]},\n",
        "    },\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Assignment-3_WithoutAttention\", entity=\"karanwxlia\")\n",
        "\n",
        "wandb.agent(sweep_id, train, count = 200)\n",
        "\n",
        "\n",
        "#train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment3-Seq2Seq-NeuralMachineTranslation.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7fdc5f3cce225da97ecbb62baa9d502a10bee8c564318ad754c1ff5e9f74492"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
